=======================================
6/14/2021

Up to dealing with memory and arrays in the Type vars.
Missing a grammer to type the state of memory.

Grammar:
 *  type = tcon | tvar | tfun[?] | tstruct[?] | ttuple[?] // Types are a tcon or a tfun or a tstruct or a type variable.  A trailing ? means 'nilable'
 *  tcon = int, int[1,8,16,32,64], flt, flt[32,64], real, str[?]
 *  tfun = {[[type]* ->]? type }   // Function types mirror func declarations
 *  ttuple = ( [[type],]* )        // Tuple types are just a list of optional types;
                                   // the count of commas dictates the length, zero commas is zero length.
                                   // Tuples are always final.
 *  tmod = := | = | ==             // Field assignment modifier; ':=' or '' is r/w, '=' is final, '==' is r/o
 *  tstruct = @{ [id [tmod [type?]],]*}  // Struct types are field names with optional types.  Spaces not allowed
 *  tvar = id                      // Type variable lookup
// Nested type var, for repeats in the same type expression.
// If 'id' is used and not an external tvar, then its treated as a free TVar.
// If 'id' is assigned, then it shadows as a free TVar.
// If 'id' is not top-level, then it never escapes the local type expression.
 *  tvar= id=type
// Arrays, no length.  Layout is taken from elem.
 *  tary = []elem
// Memory types can be followed with a tmod; this covers the entire object.
 *  tary = []=elem
 *  tstruct = @{ [id [tmod [type?]],]*}=
 *  ttuple = ( [[type],]* )=

// Some common primitives typed
Array-load :  {[]} : { []==elem int      -> elem } = ...  // array is read/only
Array-store:  {[]=}: { []  elem int elem -> elem } = ...  // array is read/write
int-add    :  {+}  : { int int -> int } = ... // no memory effects
flt-add    :  {+}  : { flt flt -> flt } = ... // no memory effects
            sys.cp : { []==elem int []elem int int -> 0 } = ... // src array is read/only, dst array is read/write; nil return


// Some more syntax, for actually doing the arg lineup & typing in 'aa'
{+} : { int int -> int } = { x y -> $Prim.AddI64.make(x,y) } // no memory effect on type

// Ponder oper fcn name using '_' as placeholders for args.
// Also doing the special FFI syntax to talk to java.
{_[_]} : { elem[]== int -> elem } = { ary idx -> $LValueRead.make($ctrl,$mem,ary,idx) } // read-only memory effect on type
{[_]} : { int -> [] } = { ary idx -> $NewAry.make($ctrl,$mem,ary,idx) } // write-only memory effect on return type
{_[_]=_} : { elem[] int elem -> elem } = { ary idx elem -> $LValueWrite.make($ctrl,$mem,ary,idx,elem) } // since no '==', updates memory

  // Variations on typing arrays in aa code
  [n] -- untyped array of size 'n'
  [n]:flt[] -- flt array; "new float[n]".  Preferred; asserts result is typed as flt[]
  [n]:flt -- special case syntax, ":flt" binds to "[]", and implies "flt[]".  Ambiguous with the below.
  ary[n]:flt -- does an array lookup, and types the result as 'flt'.  Ambiguous with the above.
  [n]:[flt] -- NO: another syntax choice, can be confused as claiming the indices have to be of type flt.
  [:flt n] -- NO: a little odd; unambiguous grammar
  ary[:flt n] -- NO: unambiguous, loading from an array of float.
  flt[n] -- NO: injects a type without a ':'

Now need type var syntax for exposed free tvars.
Have not decided between these two syntaxes:
  Tree A = : @{ left : Tree A ? , value : A, right : Tree A ? }
  Tree<A>= : @{ left : Tree<A>? , value : A, right : Tree<A>? }





=======================================
6/8/2021

Still combining value & live.

Value lattice: Top/Any, con(1,2,3), Bot/All
Live  lattice: Dead, Live

xfer-function for primitives:
  // Issue is: for most primitives, it makes no difference
  // Prop live first, then values
  if all uses dead :  (top    ,dead)
  else:
    if any defs Top:  (top    ,dead)
    if any defs bot:  (bot    ,live)
    if all defs cons: (op(con),dead)
  if Stop : always live
  if Start: always true

  // Prop values first then live
  if all uses dead or con-not-Stop:
    if any defs Top: (top     ,dead)
    if any defs Bot: ( ?      ,dead)
    if all defs cons (op(cons),dead)
  else some uses alive:
    if any defs Top: (top     ,live)
    if any defs Bot: (bot     ,live)
    if all defs cons (op(cons),live)
  
 true     true     true     top
Start  -> ID1  ->  ID2  -> Stop
 dead     live     live    live


Example where value<->live impact each other around Loads of fcn ptrs.
Issue is a fat-pointer (so requires a fat-live) with a #fidx constant
and a display ptr.  Basically, a dead linked-list load.

  (mem0,x0) = New*12(fld)      // New object, returns memory & pointer
  fib0 = fptr(x0,#47)          // Construct TypeFunPtr from a display pointer and code constant
  mem1 = st.fld(mem0,x0,fib0)  // Store into display
  call(*47)(mem1,x0)           // Top-level fib call

  fib() {                      // Function entry
    mem2= parm(mem1,mem2)      // Memory  Parm from entry and loop-back
    x1  = parm(x0  , x2 )      // Display Parm from entry and loop-back
    
    fib1 = x1.fib;             // Load TypeFunptr
    x2 = fp2disp(fib1)         // Extract display pointer
    fib = fp2fun(fib1)         // Extract code ptr
    call(*fib)(mem2,x2)        // Recursive (looping) call
    cepi  ...    
  }

// Start GCP
  (mem0,x0) = New*12(fld)      // mem0: (fld=T )/(fld=X)  x0: *12/X -- Starting, so computes value despite dead
  fib0 = fptr(x0,#47)          // T / X
  mem1 = st.fld(mem0,x0,fib0)  // T / X
  call(*47)(mem1,x0)           // 

  fib() {                      // 
    mem2= parm(mem1,mem2)      // T / X
    x1  = parm(x0  , x2 )      // T / X
    
    fib1 = ld.fld(mem2,x1)     // T / X
    fib = fp2fdx(fib1)         // T / X
    x2 = fp2disp(fib1)         // T / X
    call(*fib)(mem2,x2)        // T / X
    cepi  ...                  // T / L -- Stopping, so computes live despite Top inputs
  }

// Some GCP progress on live, some backwards flow
  (mem0,x0) = New*12(fld)      // mem0: (fld=T )/(fld=X)  x0: *12/X -- Starting, so computes value despite dead
  fib0 = fptr(x0,#47)          // T / X
  mem1 = st.fld(mem0,x0,fib0)  // T / X
  call(*47)(mem1,x0)           // 

  fib() {                      // 
    mem2= parm(mem1,mem2)      // T / X
    x1  = parm(x0  , x2 )      // T / X
    
    fib1 = ld.fld(mem2,x1);    // T / X         // Still dead; both users may-be-constant
    fdx = fp2fdx(fib1)         // T / L         // Backwards from call; may-be-constant, so inputs are unused
    x2 = fp2disp(fib1)         // T / X         // Backwards from call
    call(*fdx)(mem2,x2)        // (T/L,T/X,T/X) // Call special: live-use requires code-ptr, but not memory nor display
    cepi  ...                  // T / L
  }

// Some GCP progress on value, some forwards flow
  (mem0,x0) = New*12(fld)      // mem0: (fld=T/X)  x0: *12/X -- Starting, so computes value despite dead
  fib0 = fptr(x0,#47)          // 12:(fdx=#47/X,dsp=*12/X)
  mem1 = st.fld(mem0,x0,fib0)  // 12:(fld=(fdx=#47/X,dsp=*12/X)/X)
  call(*47)(mem1,x0)           // 

  fib() {                      // 
    mem2= parm(mem1,mem2)      // 12:(fld=(fdx=#47/X,dsp=*12/X)/X)
    x1  = parm(x0  , x2 )      // *12/X
    
    fib1 = ld.fld(mem2,x1);    // (fdx=#47/L,dsp=*12/X)
    fdx = fp2fdx(fib1)         // #47 / L       // 
    x2 = fp2disp(fib1)         // *12 / X       // 
    call(*fdx)(mem2,x2)        // (#47/ L, (fld=(fdx=#47/X,dsp=*12/X)/X), *12/X) // Call special: live-use requires code-ptr, but not memory nor display
    cepi  ...                  // T / L
  }

// Turns out, this is the standard forwards-flow of values with constants
// stopping liveness... but liveness not impacting values.  Simple ordering
// of 2 unrelated global opts.



======================================= 6/6/2021

Optimistically combining values & liveness failed.
Example straight-live graph during GCP:

 true     top      top     top
Start  -> ID1  ->  ID2  -> Stop
 dead     dead     dead    live

Since ID2 is passed and computes ANY, which may_be_constant, ID1 is not alive.
Since ID1 is DEAD, ANY is passed to ID2.
The answer is stable, but wrong: the 'true' should pass from Start to Stop thru the IDs unchanged.

I allow value to impact live: constant values need no inputs, so their inputs are dead.
But never can live impact value, or I get versions of the above example.


=======================================
5/12/2021

Fun:   gather of control
Parms: Lambda args def-spot.  Feeds into New directly, and uses inside the Lambda are never fresh.
- The Env pushes a VStack of the parms.  These are all the not-fresh vars.
Load: Drop the 'make fresh on removal'.  Do nothing 'Fresh'.
- In parser, when doing name lookup, insert a Fresh after Load.
FunPtr: is the value of a Lambda, not the variable being defined.  No fresh issues, but behaves like a Lambda.
Stmt:  Behaves like a Let; use push nongen inside the statement def; pop nongen afterwards.
- If after ifex() not a fref, no push/pop nongen.
- If after ifex() was a fref, pop nongen
Fact: If fref, push nongen (once; if not already pushed).
Func:  Behaves like a Lambda; push nongen scope; add vars for each parm; all are 'popped' when Env is popped


Fun                            
  Parm
  ...
  Ret
FunPtr                    Lambda: make a Fun of Parms->Ret

Call (FunPtr Args...)     Gather Args
CallEpi                   Apply: Fun+Args unify to FunPtr




=======================================
4/14/2021

Back to HM algo.  Missed 'fresh' a bunch; 'occurs_in' was not right.

Thinking:
- Cut Parse back to the nubbins; no 'xform' and no 'inline'.
- Confirm get HM answer correct from HM test cases.
- Supporting mutual-letrec.
- Loads from Display just like HM.Ident; and needs 'nongen' set.
- Stores to Display, very similar to '?:' just use same tvars.
- Load/Store from Object/Array use memory/pair-like defs.  No recursive defs?

- Need to build a nongen set during Parse.  This is almost exactly the
  env._scope.stk() display, except during mid-def of recursive and mutually
  recursive functions.
- - WithOUT recursion, current Display is like a nested set of 'let x= ... in ...'.

- May have to look at Unresolved as a case of Haskell Type-classes.
- - {+} {int,int->int} {dbl,dbl->dbl} {flt,flt->flt}?  {str,str->str}
- - {==} {int,int->bool} {dbl,dbl->bool}, etc....
- - Haskell TypeClasses are declared interfaces, not duck-typing.

- Once TV2 produces correct answers, start turning on XFORMS.

- Besides turning OFF XFORMS, what else can i keep?
- Can i keep NewObj for Displays & add-in the needed let-rec support?
- - Add in support for ordered sets of tvars, used in the nongen tests?



=======================================
2/23/2021

Back to HM algo.  Missed the 'fresh' notion in main code: let Name = Body in Use;
If   Name is in Use  then use a Fresh let result;
else Name is in Body and  use the     let result.

In main code; Names are use-def edges (SSA).
Typically want FunPtrNodes to deliver Fresh,
except when recursively self-defining.

So when looking at a FunPtrNode, tvar behaves "as if" wrapped in a Fresh.
Here's maybe the bug: At a normal Call/Cepi with a FunPtr, I do the
fresh-unify.  But if the FunPtr is passed along (and merged with others), I
lose the Fresh notion.  All unification points (args to functions, Phi nodes),
need to honor the Fresh notion.

Also, lots of replicated code because TMulti does not support sparse edges (ala
TMem aliases), nor named fields (TObj).  Replace TMulti with a sparse array
indexed by Object (either String or Alias index/Integer).  Drop TFun, TMem,
TArg, TRet.  Add a String for a structural-type, for asserts.  Add a Fresh wrapper.
Normal unify splits into fresh/not-fresh as HM does.
Drop unify_lift & debug until i can run all tests except HM15.

Can i get rid of TNil & TVDead?
Fold TMulti and TVar?
In HM, using _updates; in TVar its called _deps, and should only take CallEpiNodes.
Still keep _ns for debugging; but only for debugging.

Keep new TVar counts under control? not getting big in HM.  Obvious: add a
string per alloc side, & histogram allocs.  Keep with TVar, and on unification
also histogram its death (no GC, so cannot recycle, without refcnt).


Make a TVar2/TV2; and clone HM.java behavior as much as possible.


=======================================
2/16/2021

Mid-integration with H-M.  New situation with Memory.

H-M assumes all args are immutable, visible, and the return type is a function
of args (and the fdx).

Flow-types assume values are immutable but memory is incrementally updated
(mutable), with new aliases in-function and also field updates.

Integration pretends memory is both an arg-in and a return-out.  It is split by
escaping aliases; non-escapes are NOT arg-ins; escapes ARE arg-ins.  The
return-out memory is a function of the arg-ins, including escape-in memory.

Precision on the flow side is at the alias level, but needs to be at the FIELD
level.  Precision on the H-M side is at the field level.

Bug#1 is: MemSplit & MProj did not unify.
Bug#2 is: body of fcn never lifts Parm:mem/Split/Join/MrgProj to TMem... so MrgProj never reports new-in-fcn TObj.
----

Want a TLazyMem: has a prior TVar, a BitsAlias and a TObj.  Unifies the TObj at
all BitsAlias in the prior TVar - once it becomes a TMem.  Can just stay apart
if prior is a TLazyMem, buggy if anything else (e.g. TFun).  Two TLazyMems can
unify TObjs if both BitsAliases are the same.  A TLazyMem and a TMem can make a
new TMem by copying the old TMem except at aliases.

Want all Nodes to produce an initial Txxx that is sharper, and not require
'unify' except once (per original H-M, but more times with partial-eval
modifying program shape).

Want a TLazyObj: has a prior TVar, a field & a TVar.  Merges fields.  But
fields do not flow forever, nor get huge in number.  So just make TObj allow
for lazy new field to appear?  eg. a Load/StoreNode has incoming memory
as a plain TVar (not TMem), or incoming obj as a plain TVar (not TObj).
Will force output as a TLazyMem with a TLazyObj (plus named field).

For a FCN, the input mem parm will be a TLazyMem w/no aliases except what
appears inside the FCN.  Fresh-unify will unify with the TLazyMems, and require
the not-mentioned aliases be the same before & after the FCN.  E.g. a
TFun = { [CTL:TVar MEM:TLazyMem[exceptions] ARG3:TVar  ARG4:TVar] -> 
         [CTL:TVar MEM:TLazyMem[exceptions] REZ :TVar] }
Forces the not-exception memory aliases to be the same.


----

H-M alternative: split memory by aliases (and by field within alias), as
explicit top-level TVar args into a Call, or top-level TVar returns.  New-in-
call is treated as a top-level input (of UNUSED) and a top-level return.  All
side-effects are modeled as part of the memory-return.

Aliases that are neither esc-in nor esc-out are not incoming args nor can force
fresh-unify anything.

CallEpi are the Apply return-points, and unify "as expected": defaults to a
TRet which has a TLazyMem to-be-unified with the return ala fresh-unify.

Why a TLazyMem instead of eagerly chasing all TObj/aliases down and flattening
into a TMem?  Because we *never* answer the question of unrelated aliases in an
unrelated function.  It's the variable never asked for.

Which brings me to: Tmem becomes lazy by-default always.

TLazyMem: has a default, has a "you asked for it once before, so i got the
short-cut answer right now", has a "exceptions, not the default".  The
effect is: "whatever the default is, unless looking at exceptions".
When fresh-unify, the default can bottom out at "no default given" which
might be a plain-tvar.  The Parm:mem & Ret:mem can have the same default,
which fresh-unify uses to force before-mem and after-mem to be the same.


=======================================
2/2/2021

'remove_ambi' situation: no high fdxs, so no *optimistic selection* fdxs.  so
double-loop in GCP not needed; no remove-ambi.

But concept of needing remove-ambi still valid: optimistic selection of choice
overloads.

2 kinds of overloads: (1) from type-specialization, with a clear monontonically
improving choice, and (2) unrelated primitives (+:int vs +:str) with conflicts
and (2a) unrelated primitives with cost (+:int vs +:flt).

Current issue: during GCP, Call.live_use is declaring unwired fdxs as "will be
wired", so Args "might-be-live on the to-be-wired fdx".  Never wired, because
call is in-error.  "good_call" does not distinguish between "some args high, so
waiting for them to fall" from "bad nargs or bad args".  During GCP want to
distinguish between "CG still building" from "CG is built, but errors so no
longer want to wait for CG"

Call old resolve code helped here.
GCP args falling from high.
FDXs have some overloads; Scalar->Int type-specialization, or
same-name overloads +:Int-vs-Flt or +:Int-vs-Str.
Ex ARG lattice: TOP->Int/Other->Scalar.
Ex FDX lattice: TOP->50/54->[50,54], where [50] allows Scalar and [54] allows Int.
Want CEProj to never hit CTRL on paths that will never take.
[Arg:Top,FDX:Top  ] -> 50:X,54:X
[Arg:Int,FDX:54   ] -> 50:X,54:Ctrl
[Arg:Int,FDX:50,54] -> 50:X,54:Ctrl
[Arg:Str,FDX:50,54] -> 50:Ctrl,54:X
[Arg:Int,FDX:50   ] -> 50:X,54:X // ODDLY, staying HIGH waiting for 54 to appear.  If it does NOT appear, needs to fall.

This is the remove_ambi situation: call stays high (disallows all FDXs) until
no-more-progress.  Then GCP calls remove-ambi, which starts allowing FDXs that
are available, but can be filtered.  Which means no FDXs until either
remove-ambi, or i can tell this FDX will never be overloaded.  Or Unresolved
puts out high FDXs (which fail at Phis, but can all fall down) (high FDXs is
what i was doing).  Call resolve allows no high FDXs until resolve-ambi, then
the call is flagged to 'fall this way', or can prove exactly 1 high FDX will
ever pass.


=======================================
1/31/2021

Maybe a breakthru on Call+resolve+Unresolved.

FunNode has a TypeFunSig.  This can FALL during ITER (lift during GCP?) as arg
uses die (during GCP all args start not-live so all TFS can be super-low).

A FunNode validates each path independently, and sets a bit-per-path.  (Part of
FunNode value?  Add a CProj on every wired Call path?).  Valid means that all
args are "isa" the TFS.  Invalid paths mean the call+fun is in-error, normal
during Iter.  They still push results thru (despite being invalid), since the
TFS might FALL to drop the incoming arg requirement; dead args can be Err.

Parms merge all paths, including in-error paths & overload paths.  If the arg
fails to be "isa" the formal, Parm drops to ALL.  "The poison spreads".  This
is monotonic; if an error arg dies the path remains valid & the merge is
correct (and the FunNode no longer is in-error).

Calls just pass along the FDXs, no resolution filtering.
CallEpis wire all FDXs unconditionally.

Overloads then might wire to all options.  Most options would be in-error.  As
args lift during Iter, we can unwire overloads.  We ask the FunNode if this
overload path is valid.  Also, if FDXs lift to fewer, we can unwire.

Ambiguous error if more than 1 overload remains.

We keep all paths during Iter even with BAD args (because TFS might lift).  We
also keep HIGH args, but these do not pass along much info.  Post GCP#1 we have
all wired+extras (might require GCP#1 to resolve totally unknown FPTRs, so no
guarentees out of Iter#1)

THIS IS WORKING OUT.

Think I need a Call Graph edge; a CEProj between Fun and Call, so individual
edges can be turned on and off.  GCP needs optimistic edge discovery to resolve
cyclic overload fidxs.  Theory is i've wired a call to {+} to all 3 {+}s;
during GCP i discover that (a) all start high, so ok, (b) one arg falls to 1,
which flows to {+}:int, which around again.  Need to not allow {+}:str to fire
until all args are not-above-center() and also isa_formals.

CEProj.value: XCTRL while Call is ANY/XCTRL; FIDXS are high; ARGS are high;
Then and only then, look at the exactly trailing FUN, get _sig, and check args
being 'isa'; allow CTRL is all are OK.
Parm.value: ignore _sig isa; just use the CEProj state.


=======================================
1/28/2021

Maybe a breakthru on Unresolved....

Have a way to specify that "this collection of fcns are ambiguous", maybe
always true based on name alone (e.g. {+}).  Then any BitsFun is a collection
of bits (as normal), but some of them may be ambiguous with some others (but
not all).  They can be both high and low BitsFuns, as normal (and mixed with
other bits).  Normal forward flow rules, except at a Call.  Call filters out
BAD arg choices as normal.

LEAST_COST, during Iter same as Opto: can only be used once no more other
lifting can happen.  Standard problem: BAD args can go dead and lift to ANY,
so cannot prematurely remove a FDX with BAD args.

Need to expand on the TestMonotonicCall.  Add ANY args.
Can I ever filter out FDXs?

When an arg dies, a FunSig LOWERS to ALL (allowing even an Err argument).
Previously illegal args can become legal.
Iter: Args keep lifting.
Unresolved makes a low set of FunPtrs.
LEAST_COST tosses out the higher cost, once its clear there is a lower cost legal FDX.
Can be run at any time.

i.e. Call.value: Keeps all FDXs until there is a lower cost one; keeps BAD ones
(in case ARG goes dead), keeps LOW (in case lifts).  Keeps HIGH (might as well)

Parm filters by valid args, keeps at ALL until valid (can keep at ALL until all
Parms are valid; dead args same as ALL, always valid).

Opto: Args keep falling.
Unresolved starts with a high set?

Illegal FDX

A FunSig starts high (all args dead, all calls allowed? not monotonic?) - Seperate question.



=======================================
1/11/2021

Restructure of GVN in progress.

=======================================
1/6/2021

Got incremental H-M working.
Got decent evidence of 'monotonicity' of unify.
For lattice, TOP=a shared TVar (all part of it), BOT=independent TVar.
H-M starts @ BOT, and unifies towards One-Shared-TVar.


Thinking of reworking GVN iter, to get me more structure & control over it.
Seperate worklists for dead (instead of recursive kill0), for strict removal
(eg, CSE, folding +0, st/ld), flowing {value,live,unify}, strict same edge/node
counts but lowers something, increases edge/nodes (but maybe adds parallelism),
larger simple xforms, inlining (maybe sorted by utility).

Seperate ideal_xxx calls for each of these, along with seperate worklist_xxxs.

A way to gather neighbors for each ideal_xxx or value/live?  For a
value/live/unify, return the new value plus also a "neighbors used to make
this".  Store with each Node the reverse-neighbor list?

How about:
  old.add_work_flow (nnn);  // called iff value/live changes, adds to value/live worklist
  old.add_work_unify(nnn);  // called iff unify changes, adds to unify worklist
  old.add_work_reduce();    // 




=======================================
Notes from 12/28/2020

Back around to basic theory.  Progress since last time: pushing H-M through
most code, but then up against some basic stuff.

MEMORY: Suppose I treat a H-M "pair" as memory, extending the 2 elements to N.
As long as the index is a fixed constant (e.g. car,cdr on pairs), the H-M
updates are fairly obvious: A "car!" or "cdr!" forces unification of the
before-and-after.

Example: pair (X,Y), "car! (X,Y) Z" results in (Z,Y) and unification of {X,Z}.
If I extend car/cdr to a "store" with an unknown index, everything the index
touches must unify.

Example with a 4-wide "pair": (A,B,C,D), and a ptr with range 1-2, and then "st
(A,B,C,D) ptr Q" --> (A,B&Q,C&Q,D) (maybe updates B or C) and forces
unification of {B,Q,C}.  Later during "iter" ptr lifts from 1-2 to the constant
1, and the store result lifts to (A,B&Q,C,D) and we should LOSE THE UNIFICATION
of C and Q.  This is not monotonic!

Same basic problem as dead-code on a test: (P ? A : B) unifies A and B, but if
P becomes a constant this unification should be lost - and is not monotonic.

This leads me back to Click thesis chpt3 - a need to express unification rules
as equations, in order to get the basic theory right, and get back to some kind
of monotonic solution.

---Attempting to make a pair-of-nodes Unification Lattice:

Using CCP from Click thesis directly, including jargon Lattice_c (for integer
constants) and Lattice_u (for (un)reachable), shortcut: Lc and Lu.

Lattice_unified (Lf): two Nodes are unified or not.  This is a 2-D bitwise
relation.  We hope it becomes an equivalence class, with a distinguished
Leader.  (As of 12/28/2020, seeing the need for a Leader & Followers).

(X<=>Y) are unified if:
      (X===Y)         +        // Reflexive
      Y:=X            +        // Y is copied from X, includes Loads & Stores & Projections of Tuples.
                               // And Y is marked as a Follower
      (X_isa_Phi)&(path#1 reachable)&(X[1]===Y) + // Phi and reachable path are unified
      VX=TOP          +        // Either is Value TOP
      VY=TOP          +        // Either is Value TOP
      VX=VY=c0        +        // All compute the same constant
      ....

Lattice_f does not hold onto the notion of Functions or Pairs or Oper.
In H-M, Nodes have a constructed T expression which is shared with other Nodes:
 - A unique numbered TVar; the same TVar can appear many times in a T expression
 - A TMulti fixed-length tuple of TVars, indexed by constants
 - A TArg, which is a TMulti of length 3+, [Ctrl TMem Arg1 Arg2...]
 - A TRet, which is a TArg of length 3
 - A TFun, which is a TMulti of a TArg and a TRet
 - A TMem, which is a TMulti of length #aliases (program constant) of TObjs
 - A TObj, which is a TMulti indexed by field name (one-to-one mapping from field name to field index), which hold TVars
 - A TAry, which is a TMulti with a single element TVar (someday maybe the size).

Typical equation-solving gives a unique TVar structure to *a Node*, but H-M
solving gives a shared TVar structure, but this is for efficiency and not
required.  

Each H-M unification step either (1) succeeds and maybe grows the 'leaves' or
(2) fails, and the program is in-error.  Success adds constraints to a T
expression; if the sub-parts are shared then some other Nodes' T expression
also changes.  If we do not share, then instead Lf is a per-Node T expression
which monotonically grows.  This means we are computing a T expression
per-Node.

Given a T-expression & a Value type, we can force alignment by doing a JOIN
on Leader only (Followers get Values flowing from Leader).
Which turns into, we can apply the H-M "fresh_unify" notion in the old Type
system at Calls.



Need to make a Lattice_f: define a 'meet' and 'dual'?
Assume all lattice elements are 'below center' for now.
A 'TVar' is essentially a 'lattice bottom' and can lift to any more specific type.
Phi xfer function = { ctrl_1:Type.CTRL val_1:Type hm_1:TVar
                      ctrl_2:Type.CTRL val_2:Type hm_2:TVar ->

                      if( ctrl_1==X && ctrl_2==X )
                        return [Type.ANY, plain TVar];
                      if( ctrl_1==CTRL && ctrl_2==X )
                        return [val_1,hm_1] // same value type, same hm as LHS
                      if( ctrl_1==ctrl_2==CTRL )
                        return [val_1 MEET  val_2,
                                hm_1  UNIFY hm_2 ];
                    }


Need to define unify:
- Given 2 complex structs with equal name&arg.length
  - its the structural recursive answer
- Given a TVar & anything else, its the other thing
- Can be a base type
  - Given 2 base types, use TYPE system.
- Otherwise its an TError
This implies a lattice: plain TVar at TOP, complex in the middle, TError at bottom.
Dual can be just replicated 'above' & ignored till GCP.

Exploring this "lattice" with shared TVars -
  { x -> x } MEET { 3 -> y } ==> { 3 -> 3 }

  { a -> { b -> c } }  MEET { { d -> e } -> f }  ==> { {d -> e} -> {b -> c} }

  { [a] {a->b} -> [b] } MEET { [int] {c->flt} -> d} ==> { [a:int] {a:c:int->b:flt} -> d:[b:flt]}

  { bool a a a -> a } MEET { b b b b -> b } ==> all bools

  { x -> x } { (a,b) -> (d,e) } ==> { (ad,be) -> (ad,be) }
LOOKs like lattice to me!!!

Ignoring shared TVars, this could be a refinement of Type system.  Assuming
IMMUTABLE, with marked shared (un-interesting differences are always
hash-consed, shared, immutable).  Shares are unique per expression, and must be
top-level marked, or else cannot tell shares here from shares there, and we
might get cross-TVar sharing (which is buggy).

Summary: need a top-level THM which delimits TVar sharing.  Internally plain
TVars are null; shared ones unique numbered from 1.  Still need to look at
Apply / Call+CallEpi.

Lambda/FunPtr xfer function = { arg:Type tvarg:TVar ret:Type tvret:TVar ->
                                return [TypeFunPtr(arg,ret),
                                        TFun(tvarg,tvret)];
                              }

Weirdness at Let,Apply, because of UNIFY.

ALSO: could be a bug with nongen missing contents of memory

======================================= Notes from 11/23/2020

Blending H-M & Flow/Lattice Theory.

Adding H-M constraints "looks like" making two Nodes structurally-equal (both
are primitives, or both function-pointers, or both structures with same fields,
etc).  Thus when H-M U-Fs two TVars together, their matching Nodes should have
structurally-equal types - which I can make by doing a JOIN.

If, during Iter & types lift, I declare some code dead, it no longer U-Fs
sensibly.  i.e., what does "structurally-equal to dead" mean.  Example: "P=0; P
? A : B", might start with A & B unified, but then A goes dead and B keeps
being unified with A... and A might have had structure, which then got forced
onto B.  Dropping the U-F of A needs to drop the structural unification.  More
Example: "A = (C0,C0); B = (D0,E0)".  Unifying A & B forces equivalence
between D0 & E0.  When A goes dead, what to allow D0 & E0 to once again be
seperate.

During GCP, is something is currently dead, it no longer JOINS, so CEPI types
drop.  If something goes live, it does JOIN, so CEPI types lift.  Not monotonic.
Cannot use JOIN.

Conflict is: More H-M U-F "lifts" in that it forces equiv structural types,
which maps naturally to my JOIN.  But dead-code breaks a Union effect (or else
should allow Union to DEAD), which "lowers" resulting types.  This is UNSTABLE
& OSCILLATES.  If U-F with DEAD keeps the JOIN to ANY, then always fail to type
with simple DCE.

This is because I dont have a theory on the "best solution" or "fixed-point"
solution when H-M discovers structure-equivalence.  Current best theory is
every time H-M unifies, I've discovered more struct-eqv, which can be used to
lift types.  Lifting types can lift a {?:} op to one side, which then U-F with
DEAD & removes the structural equivalence between sides.

// still no fail here...
b:int = 1
c:flt = 2.3
dup = {x->(x,x)}
while(1) {
  P ? (dup 2) : (b,c); // Unifies (2,b) and (2,c); forcing (_:nint,_:nScalar)
}

BECAUSE PHI DOES NOT IMPLY STRUCT-EQV ON ARGS, UNLIKE H-M {if/else} OPERATOR!
Because H-M does not ask the forward-flow question, it only gathers contraints
that have to hold to type.  When flow gives an Error, H-M might claim things
have to be equal.  More like an Assert.

Keep DEAD & JOIN anyways.  Good progress, back to the is_even/is_odd test.

Now: Lacking a way to tell unify progress.
Still want a unify-fresh variant for performance & progress.

Now: After some progress work, figured out call fresh() in the wrong place.
Each *use* of a FunPtr gets a new fresh(), which only updates if the
FunPtr basic type changes shape.  This *should* be the same as calling
fresh() at every use (only use in CallEpi), except progress is wrong.

Ok, sorted out more progress.


=======================================
Notes from 11/16/2020

Got HM unification "working" on Sea of Nodes, meaning doing unification but not
using the info to do any improvements.  Also, no distinction between Lambda and
Let.  Actions:

Make a "unify" call for GVM.  Set to NOP by default.
Call it during iter().  Can do Ideal() for now.
Run "standard" unification, like i do in the constructors now.
Add a "progress" - which forces re-unification.
If a Call directly uses a FunPtr, get a fresh copy for unification.
Update the fresh copy if FunPtr makes "progress", which means it unifies with anything.

Eventually, once the TVars are correct for toy example, use the TVars to
improve CEpi return types after Call.


======================================= Notes from 11/10/2020

Branch HM has a Hindley-Milner-like thing, set for a Sea-of-Nodes (or at least
SSA), a worklist-style algo with monotonicity properties (for including in the
iter() pass), and recursive types (no occurs-check).  Missing minification of
recursive types, like TypeStruct does.  Also, the TypeVar relation changes
dynamically (at the moment), which precludes using the immutable persistent
Types I do now.

Concept: TVars map to a Node, and a collection of U-F'd TVars map to a
collection of Nodes.  The collection, as a whole, is a MEET, but individual
Nodes keep their Type (JOIN vs the whole).

Concept: This is the SESE principle at work: args into a Call have a Type
relationship amongst themselves (and the CEpi), but the CEpi result is lifted
from the merged Ret value.  Types in a Fun/Ret are merged & approximated,
but sharpened again at a CEpi.  The TVars map those relationships.

Concept: TVars have structure like Types: TStructs, TArys, TFuns.  We can argue
TMPs, TFPs, TMems as well.  The difference is that they all bottom out at
TVars, which bottom out at a Node.


Ponder: making TypeVars immutable, so can hash-cons.

Ponder: Need syntax for TypeVars in the type sub-language.  Classically single capital letters.

Ponder: Simplistic in execution, then optimize.  No hash-cons.  Entire cut-n-
paste clone of type/Type*.java, including the cyclic stuff in TypeStruct.

Ponder: Test code for TypeVar, including cyclic unification.

Ponder:
Every Node has a TVar, which can be "sharpened" via U-F.
Every TVar also is a U-F, including "Shape" TVars.
The "base" TVar is either tied to a Node (hence also Type).
Shape TVars: TStruct, TAry, TFun.

Shape TStruct - a map from field names & numbers to TVars.
  Sometimes its just field name from a Load/Store, sometimes a number.
  Sometimes we can union a field name and a number (e.g. a constructor with ordered field declarations).

Shape TAry - just a TVar for the size & elements.  Size is optional.  Maybe use same impl as TStruct, with eg field names "#" and "[]".

Shape TMemPtr: A ptr to TStruct/TAry (or both, as a TObj).  

Shape TMem - A map from TMemPtr to TStruct/TAry/TObj.

Shape TFun - Straight from H-M: a collection of args & and return.


=======================================
Notes from 09/07/2020

Short-circuit evals.

Plan A: Thunk the RHS expr & pass the thunk to all binops.
Prims like '*' unthunk it.
Prims like '&&' do short-circuit eval & unthunk on demand.
Allows user to define new short-circuit operators.
Means expr() will thunk all term()s independently, except perhaps the first.
Then precedence determines the order of eval.

Example: a && b || c && d
Assume a,b,c,d have side effects.
if( a() ) {
  if( b() ) {
    returns b
  }
}
if( c() ) {
  if( d() ) {
    returns d()
  }
}
returns 0

During expr parsing, once a thunking operator is found, thunk all remaining terms.
NEEDS: a way to 'thunk' a term(), where a thunk is a no-arg function with all callers known.
When combining terms, if RHS is a thunk & operator is no-thunk, then "dethunk" it.
NEEDS: a way to 'de-thunk' a thunk; simple inlining.
Inside the thunking operator, expect an IF around a eval'd thunk.
NEEDS: a way to 'de-thunk' in the graph, expected AFTER operator inlining.
After an op return, needs a way to 'thunk' it.



=======================================
Notes from 09/01/2020

REPL- Lots of cleanup/improvements already.
Think I'm needing a use-driven inline/clone.
FunNode with REPL usage & unknown caller & REPL-use Call clone to remove unknown.
Call has to resolve to clone.

Thinking-
- Change of plans.  Drop back-flow of REPL.  Use new cloning strategy: clone
  always if using REPL & has a default input & dropping the default changes sig
  & found from other resolved calls.
- Ponder nested cloning (ugh, tree-shaped cloning or risk endless cloning for
  ever-more-minor variants), where a Fun with multi-inputs can split?
- Cleanup & keep merged live state.  Means I can reverse-flow some flags in the
  future.
  
- Clone always has a default input; limit to sig improvements, although even
  minor sig improvements might be good?
- GCP/Opto.  Both defaults & clones get a chance to resolve.  Some/(many?) of
  the clones will never get resolved-to; get cleaned up.
- Each new-line, some new Calls appear, resolve to some Funs.  
- Each new-line, visit all FunNodes, clone if sig varies when dropping default.

=======================================
Notes from 08/29/2020

REPL-

- Want no impact on error; thinking about giant assert of copy-all Nodes &
  verify after cleanup that all nodes are back the same.
- Might be OK right now, just copy/reset top-level Display.
- All funcs kept conservative with the default input.  Prevents typing.
- Ponder cloning aggressively without the default, on all call-sites LIVE from
  current exit value (as opposed to live-for-all-time).  'nuther flavor of LIVE?
- Like LIVE>>ESCP>>REPL?
  LIVE: Regular alive for future, but not needed now for result.
  ESCP: Regular alive for future, but not needed now for result.  Exits local scope .
  REPL: Regular alive for future, and YES needed now for result.  Exits  all  scopes.
  Or just walk backwards once?

- Rename ESCAPE to 4 chars: ESCP.
- TestREPL calls REPL directly.

=======================================
Notes from 08/12/2020

Yanked priv/public notion.  Failed when calling many fcns, returning the same alias many times.

Back to Basics!
Standard Call-Graph Flow from Rice.
Live & Value both entirely symmetric, so symmetric handling.

Add to Ret a MOD-OUT set (union of NEW/STORED).  Ignores read   aliases.  Forward flow.
Add to Fun a READ-IN set (union of     LOADED).  Ignores stored aliases.  Reverse flow.

Recursively expand MOD-OUT at CEPI.value from union of RETs plus local.
Recursively expand READ-IN at CALL.live  from union of FUNs plus local.

Call.value: just capture from above
Cepi.live : just capture from below


Cepi.value:
  for all rets, take pre.call or post.ret, based on ret MOD-OUT.
  meet results.

Symmetry
Call.live:
  for all funs, take pre.cepi or post.fun, based on fun READ-SET.

How does this handle Recursion?


=======================================
Notes from 08/05/2020

Trying to solve: New / Call / St=; problem is Call blows aliasing on New if recursive.
Code: "foo = { ptr -> ...@{ x=foo(ptr.fld); y=foo(); ... } }"

In general, want to solve the problem of swapping a New & Call.

(1) Via flow, but recursive return New is confused with an entry of prior New.
Easier to see with a loop:   "prev=0; for{P}{ v=f(i); prev=@{_next=prev,_val=v; }}"
So thinking:
Keep a private & public memory in TypeMem, per-alias.
Each ptr is either public or private.
New makes a private ptr.
MrgProj crushes all internal private ptrs to public, tosses away any prior
private memory, replaces it with a new private memory from New.
Private ptrs lose private at all Phis (e.g. ptr-meet is ALWAYS public).
Storing & Loading private ptrs is allowed, and private-ptrs can be in memory.

(2) Via graph, "ptr.fld" confuses with New, so cannot swap New around until
after GCP sorts out 'ptr'.  Next problem: cannot swap New until after GCP, but
the swap is required to solve the final-store of "y=".  So need to clone
"foo={ptr->...} for a variant of ptr with fld.  "foo={ptr:@{fld} -> ...}".

IDEA: Ptrs have 3-value: public, private, both.
New ptr is private.
Possible looping points (Loop-phi, Fun) sets to public, clearing private.
Other Phi merges as normal (so can be pub+priv).
TypeMem has public & private variants of all types.
Stores update or the other or both; always precise in private, always a MEET in public.
Loads from either merge, otherwise just load from one.
MrgNode resets private mem to the New, passes-thru public.
Fun/Loop/Phis all preserve private memory.
Calls do local-esc analysis for CallEpi.
CallEpi keeps pre-call private memory if not escaping, else uses post-call private memory.

META-IDEA: partial unroll of graph in flow, to get the precision without cloning.



=======================================
Notes from 07/31/2020

- bring back "MemMerge"; really need private-vs-public *memory* and pointers.
- pointers i can declare private via graph shape: direct ptr to DProj-then-NewNode.
- for memory, i need a node which MEETS public & private memory, unlike Join
  which knows it has absolute independence and does a STOMP.
- This MemMerge can really replace the MProj after a New, and so be a MrgProj.
- MrgProj inherits from MProj, adds a public-memory edge and MEETS it.
- MrgProj can flip unrelated public memory to its other side, similar to Join,
  pushing other ops into the "NewNode/MrgProj" region, effectively growing the
  known-private-memory region.  Goal is to handle a version of MAP which NEWs
  before the recursive call, but to keep the known private version after the
  MAP to allow private updates... and supports proper inlining.  Cloning a New
  makes two children which should properly be monotonic for the "off brand"
  memory.
- New no longer takes memory, yes takes control.
- Mrg has a ProjNode like input in slot 0, and a Memory like input in slot 1.

=======================================
Notes from 06/29/2020

- Factory allocations blow all LHS/RHS choices.
- Always "go right" which means no split/join.
- Drop MemMerge
- StartMem is always ~use
- DefMem starts use, lifts per-alias as they appear.
  Eagerly updated as New is updated.
- New takes in mem (not control) and Meets with itself along alias.
- - New produces MProj not OProj
- New tracks simple local escape knowledge.
- Store takes in mem and produces mem, and meets with correct aliases
  unless a single (no children) alias, and then can stomp.
- Parallel Stores bypassing requires a parallel MemJoin/MemSplit.  Still exact (no
  overlaps nor parent/child).  Lazy added.


- use: possibly allocated to worst possible
- obj: alloc as something
- @{low open }: adding  fields; all unknown fields are 'all'
- @{low close}: no more fields; all unknown fields are 'any'
- ~@{}: high, discovery?
- ~obj: high, discovery? of struct-vs-array
- ~use: never allocated

PONDER:
Drop TypeStruct._open, use TypeStruct._use instead



=======================================
Notes from 06/13/2020

Action items
- add global-ptr-use types (used as adr, stored into mem, merged at phi, call arg)
- - meet/dual
- - value props
* - drop NewNode.escapes, DefMemNode.CAPTURED
* add struct-field-complete bit, drop BitsAlias.nflds
- add global-ptr-use live types (subclass of TypeObj, tracking reverse flow props)
* - add global-ptr-use ESCAPE type
- - live-use props
* - live-use ESCAPE props
*- OBJ: never-alloc; low-struct: alloc, still adding fields; high-struct: closed, no more fields; XOBJ: alloc, uninit; UNUSE: unused/dead
* make Call/CallEpi dumb on memory and ptrs
- add Split/Join in parser around all calls & memory uses
- - todo: start optimizing split/joins
* Store/Load do not bypass Call/CallEpi (but yes bypass Split/Join)
* CEPI takes a default RetNode value, same as FunNode takes a default Caller.
* Default FunNode caller knows about default Display memory for parsing.
* Wire when known.
- Set value equal to the default RetNode or default Parm; this allows OOO types
  right up until we remove the default.  At that time, the types must be in alignment.


Live values
- are TypeMem
- for simple numbers, use TypeLive sentinals for live/dead in slot 1
- for pointers, have use types (used by call, store-val, return, etc).
  The live-pointer-value does NOT carry alias info, nor used fields???
- For memory, uses include TypeStruct fields.



=======================================
Notes from 06/11/2020

Want to get away from non-local graph updates, e.g. DefMemNode.CAPUTRED &
NewNode._no_escape.  So move these ideas into the graph flow.

Forwards: ptrs are stored (or not), hence get mixed into memory & thus into
unknown OBJ fields - or not.  A forwards flow property.  Might add into
that a ptr is ld/st address, is used as a call arg, is phi mixed.

Reverse: Ptr values are not just basic-live, but stored (or not), used as an
address, a call arg, etc.  Used by a ScopeNode gives a worst-case user,
so Parser does normal keep-alive.

Doing both of these as a Type means they just flow = no non-local graph issues.

Need a un-init memory value (ISUSED?), a OBJ memory value, a XOBJ and a UNUSED.
Want to track un-init so at split/join can precisely split aliases.

Want to precise-split memory.  No more mixed aliases, and no more mem-merge,
and no more memory meets from MemMerge.  (Imprecise stores can do meets).
Use MemSplit/MemJoin.

Want Split/Join around calls for non-escapes.  Want split/join around all
NewNodes as a single-alias precise memory split.  Want split/join around
all stores (and loads), using the ptr-alias.  This gives me a zillion
split/joins (instead of memmerges) and each one is an exact split.  Can
be obviously optimized (widened) out to the ScopeNode.

This gives the notion of having a Split/Join around each tiny memory piece
in the Parser, and optimze the crap out of it later (including optimizing
in hte Parser).  BUt get it right first, optimize later.

Need a Split tech that replaces the Call/CallEpi - so a Split varient that
takes in call args, does a full reaching analysis and splits memory based
on what reaches.  Call/CallEpi get "stupid".  Inlining a Call gets trivial
again.

Need a split-tech that takes a single ptr value and splits on it.

Join always does SESE regions, left is the Split, right is whatever - and
never do the memories overlap.  So just parallel-joins the memory.

When memory is split, label the not-available side as e.g. XOBJ.  Note that
un-init always "goes right", so 1st time creation can happen anywhere.

A little thinking on the monotonicity problem:
  Split
    Call
      Fun...Funs
        Body...Bodys
      Ret...Rets
    CEpi
  Join
Looking for the optimistic lift during GCP; if I do not find it, then I can
pre-compute - there is no phase-ordering problem.  Optimistic: ptr arrives at
Split but is not in memory, does not escape into Call, Fun, Bodys.  Memory
contents are not modified, and Join takes un-mod memory directly from Split.
If instead at Split, I make ptr go-Right, then into Body, which then escapes,
which then forces a go-Right and mods memory.  When a ptr arrives at a Split,
I need to decide if the memory goes Left or goes Right (or both or neither?).
If I make it go one side, and later the analysis goes the Other Way - then i
cannot drop the side it already went - loses monotonicity - so instead it
must "go both".



======================================= Notes from 06/05/2020

Working on escaping aliases.

Root issue is non-monotonic behavior on escapes.  During GCP, assume ptr does
not reach a call arg, does not escape, so memory slice not modified, so ptr
does not reach call.  But if later DOES reach call, then slice is passed in to
call, and modified.  So sometimes post-call uses the pre-call slice, and
sometimes the post-call slice.  These have to be monotonic.

E.g., alias#15 arrives at a Call, and otherwise is escaping (stored someplace
into memory).  But at this call site, not used at any arg, and not reachable
from any available ptr.  THinking maybe this is a non-issue now.

When can memory slice bypass a call?  If it *ever* escapes (per NewNode), safe
to assume call hammers it - even if not available from reachable ptrs.

Ok, coming around to "not caring" - if alias escapes (via NewNode escape which
limits to *storing* or Phi which stores or a Call), then thread thru all Calls.
Goal: no escape simple recursive display ptrs.  They are not call args, and not
Phi and not store-vals and not returns.  So pass around matching memory slice.

Action: drop Call-escapes.  Keep pass-in / pre-call memory.  CallEpi acts like
a MemJoin (or i insert a MemJoin/MemSplit).  Split criteria is all aliases
based on NewNode escape notion.

This gives-up a per-call slice-around notion!  Future Work!  Can obviously
improve (to using per-call per-alias smarts), but can fix displays simpler.



=======================================
Notes from 05/18/2020

PC dead, a little behind on laptop.

Thinking through the issues with splitting memory from pointers, at type-check
places (call-sites with formals vs actuals, and TypeNodes).  Thinking about all
the places i add code to do partial correctness & forward progress - makes
things very complex.

Can't make out-of-bounds ParmNodes put out their formal value for pointers,
because their pointer aliases (from several Parms) will alias their formals.
e.g. Parm x is typed *[2]->@{x:int}, and Parm y is typed *[2]->@{x:str}.  The
result has @{x:obj}.  Why not merge memory of the formals?  Because formal
memory might LIFT from actual.  I can MEET formal memory and actual memory
also.  This is lower than formal memory; same for the Parms - I cannot lift
them to formals, because the Parm:mem will be BELOW formal memory (being the
meet of actual & formal), and even if i do not meet actual - still the meet of
formals is low.

Other theory: dump out ANY/ALL for every bad result.  Any body with an ALL
input always produces an ALL (except for region/phi which can ignore some
inputs).  Error finding can ignore nodes with ALL inputs - they will be
in-error, but they are not the root error.  Much simpler logic.  CON: cannot
start doing type-specialization, until the types show up.  Means e.g.
typed-functions cannot do primitive spec until GCP proves their input types.

Surely means i need to do type-spec on MEMORY contents, not just normal args.
But still how?  2 Parm:arg come in with ptr types, but totally unknown aliases.
Their formal memories will get merged.  Kinda want to type-spec on alias#s so i
can sharpen each Parm:ptr independently.  This gets me to the notion of having
a alias# that is for a formal, and not related to an allocation site.  I can
use this alias# for all allocation sites that meet the formal spec, in addition
to the normal alias#.  Or I can just note which alloc alias#as match which
"interface alias#s".  At a Parm:arg, if the actuals are OOB the formals, i use
the "iface#" (and Parm:mem uses the iface# for formal memory).  This means an
iface# can hit a Parm:arg.  Usual story: if Parm:arg is OOB, output the formal-
including the formal iface#.

How do i "lift"-only a Parm:ptr?  If it has iface#11, then gets actual memory
12 which is IN-bounds with actual Parm:mem - flipping it from #11 to #12 is
a sideways move???  Not monotonic?

Maybe iface#s are always "below" actual alias#s?  So a meet with any IFACE#
beats all ALIAS#s?  Otherwise IFACE#s simply meet like alias#s.  Still need a
heirarchy of iface#s - or else, if any Parm has an iface# come in, it must
produce its own iface# out.  This means all recursive/loops will only be using
iface#s.  This means i can make forward progress on the loop body - but must
always use GCP to clear out.


PLAN A: All OOB types produce an ALL.  Most nodes when recieve a ALL, produce
an ALL.  All nodes when receive a bad type, produce an ALL.  "Broken graph"
produces the same type (freeze in place).  Errors don't count if a node gets an
ALL (not the root error cause).
PRO: Much simpler.
CON: Cannot make progress without valid types, especially for recursive fcns.

PLAN B: Same as now: produce valid value out always.  For Formal pointers,
invent an "iface#" like an alias# but not related to a New.  For Parm:ptr, if
OOB, produce this iface#.  For Parm:mem, JOIN the formal memory with actual,
using this iface#.  Lifts the produced memory "as if" the formal, so can make
progress before GCP.  At any Parm:arg, if any value is an iface# or not any
alias#, or alias#+memory is OOB to formal - then produce the local iface#
always.  Similar to poison ALL, except can make forward progress.


=======================================
Notes from 03/26/2020

Wiring: purpose is to shortcut args into parm-meets.
With unknown_caller, is optional since already pessimistic parm-meet.
Right now inlining (which removes unknown_caller) requires wiring.

ITER:

Never wire choices (can disappear).  UnknownNode reports choices in iter.
Means not flowing thru primitives in recursive functions?!?  Correct.
Yes wire constant-and-multi.
Always Wire as long as nargs==args; bad args might go dead so always valid.

CHANGES: CallEpi ideal does not bail out early... rolls thru the inline checks
& then wires.  Can bail for choices, dead-from-below, or mal-formed Call.
Since inline requires are checks & wire does not: ponder flipping order.  Wire
first.  Then inline if also args are good.  Only inlines wired.

CHANGE: Call must pass CTRL if args *might* be valid to that call target, even
if not valid to other call targets.  Ponder adding a assert-args shortcut
check.  Drop ParmNode bounds, since ctrl-not-on if args fail check.
CHANGE: Call cannot check args, since valid to some targets and not others.
CHANGE: assert-args DOES error check.  


GCP:
Always wire as long as nargs==args; bad args might stay dead so always valid.
Wire if no choice: (down to 1 func, or multi).
Above-center choices never wires, needs to settle down (resolve).



=======================================
Notes from 03/24/2020
MAIN ISSUE

Add to Function Default Memory, a Merge with the incoming Display.  Always
guarentees minimal display.  The Loads from external displays can optimize away
- or at least see a Phi/Parm of 2 Merges with the same memory at the Loads'
alias.

Very similar: split Memory into Display memory and Heap memory.  Force the
split to all things.  The local-var-loads can have a pre-sharpened memory.
Bigger change, and (perhaps) a easier guarantee.  "Should be the same".

---

Other bug: cannot add type-annot after a function call, and no error.
Easy parse grammer bug to fix.  See TODO in Parse main comment.



=======================================
Notes from 02/27/2020

A lot of troubles arise because need dead bad code to actually get removed from
the graph.  Currently no notion of removing dead fields, so built a lot of
complex flow pushing 'not so near' neighbors on worklists to propagate enough
info to let fields go dead.

Add a reverse-flow 'dead' notion, only useful for fields and during OPTIMISTIC
analysis.  Useful for fields because I'm not slicing seperate nodes per-field,
so there's no per-field notion of deleting dead nodes.  Useful during opto()
because a dead field does not need to be computed, so its inputs are also dead,
recursively.  There's a classic feedback path here, with monotontically
improving results.

The base iter/opto algo can work in both directions (and iter() totally does
now), just not in the base implementation: several ideal() calls push nodes
from the reverse direction).

TypeStruct:
*Remove 'clean' per struct, no need after this.
*Add a 'dead'  bit to TypeStruct fields.  This is a reverse-flow field.
*Add a 'clean' bit to TypeStruct fields.  This is a forward-flow field.
*Defaults to 'alive'.  Alive if any use is alive.  Dead otherwise.
*Roll-up 'dead'  bit to all Types as well - for use during opto.


*Node: Reverse flow alias_uses can be removed.
*Remove 'not so near' add-to-worklist.

*CallNode: Remove filter memory into FunNode.  Just pass it all.  It will be
*discovered 'clean' in the function and the original value used at CallEpi.

*Parse: Do not clear out user-struct closure field; it will go dead.

*Opto init: defaults to dead.  Exit Scope is alive, and thus its defs are alive,
*recursively.  A Node is dead if all using nodes are marked dead.  Only
*interesting during opto, because during iter() dead nodes are deleted.

*Dead Nodes only compute their startype.

SESE Call/Fun precision improvements.  Ret starts with all memory as dead, and
this pushes uphill to Fun.  Call gets a set of alive memory from CallEpi and
from Fun - but Fun/Ret does NOT get alive memory from all CallEpis.  Removes
the classic merge approx on function exit.

SESE Call/Fun precision improvents: Fun starts with all memory fields as clean.
On exit, can be used to improve precision of merged memory results passed to
callers: CallEpi on clean fields takes Call-input type.


New map_closure
Merge Parm_mem & New map_closure
... ld map_closure.x
Call [allmem+map_closure is available]
  Fun: 
  Parm mem: [allmem+map_closure]
  ... allmem is unchanged; map_closure is also clean?
  Ret 
Call_Epi: takes from pre-call memory for map_closure
... ld map_closure: mem from Call_epi, can bypass clean, gets to Merge, bypasses...




=======================================
Notes from 11/4/2019

Reached a point where need to split by aliases across phis ... during parsing,
to keep precision enough for the nScalar tests.  Experimenting with running
iter during Parse.  Works surprisingly well.


=======================================
Notes from 11/2/2019

Missing an execution model for full closures.  Ignoring type-inference or exact
syntax or even semantics, want to actually execute w/closures to try tiny
examples on lifetime management.



---
Trying the impl...
Need to load '-' from starting Scope; scope pts to:
  ctrl,mem,New
Missed; needs to point to:

CTRL (start ctrl)
 |  (start memory)  (primitives, stored as funptrs into closure)
 |    XMEM            New
 |      \        [OProj,DProj]
  \      \          /
   \-    Scope    -/

Normal 'fact' lookup turns into:
 - find Scope
 - Issue Load for field against memory & address from Scope.

Normal 'stmt' update inserts a Store:

some               (closure#17)
ctrl                  New#17
  |    some_mem   [OProj#17,DProj#17]
  |    [all-17]     |           /
  |      |         .... (any number of stores, or Phis)
   \     \         /          / 
         Scope   -/ (#17)  --/ (the ptr-to-#17)


Does Scope need "all the other memories"?  Or just the parser?
Or is the parser using the Scope memory exactly for that...
The stack of Scopes gives me a stack of memory... which is supposed
to be serialized (except for implicit parallelism from unaliased closures).
Which makes me suspicious that in fact can be aliased.

   > (inc, get) = { cnt=0; ({cnt++},{cnt}) }()

   Fun ParmMem[all-#17]
      0
      |
     New#17 (cnt)
     [OProj,DProj]
                \
get = Fun-Parm   \  <<-- requires #17 here on mem parm
            |    | 
             Load|  <<-- since uses #17
           Ret   |
                 |
inc = Fun-Parm   |
            |    | 
            |Load|
            |    \ +1
            |     |
             Store
             /
          Ret
     (inc,get) <<-- closure memory always escapes on Ret, but can go dead later
     Ret

Called from Top-level:
  Call
  CallEpi
  [ctrl,mem+#17,(inc,get)]
  

=======================================
Notes from 11/1/2019


Pondering making NewNode a single scalar field only.  Returns a TMP with single
alias#, somehow attached to the user-notion of an allocation site (plus its
clones when inlined).  Flattens the alias# space to remove the field-level
alias.  Does not help arrays?  Allows fields to die independently.  Means
I do not have to figure out any field-level opts, since the graph does it.
NO: Does not help arrays... still need 2 levels of aliasing.


  Call fcn-ptr,args
  CallEpi: wired Funs
  [Ctrl,AllMem,Val]

Only 1 single "phat" memory in any network slice.

Rules for MemMerge: All inputs are unaliased - but may share alias# if on right
is a NewNode.  Far left is the "phat" memory (includes alias#1) and others are
input in first alias# order? (so I can find easily, beats array by alias# I
think).  Never same alias twice (unless a NewNode).

Rules for StoreNode (which I highly suspect are not yet proper): Output Mem is
same alias as Input Mem.  No bypassing phat-Store-by-phat-Store based on
different alias# without direct replacement... because leaves 2 parallel "phat"
memory.  Instead, request memory split and the independent alias#s float
about.  If stores are on different skinny alias, then already bypassed.

"Request Memory Split" - if this Node expects to use some sub-part of memory,
but is given phat memory, pass the request "up hill".  If this Node expects to
"root" "phat" memory, then insert the split: AliasProj's based on users;
AliasProj's need some quick way to assert unrelated alias#s - all alias# splits
listed on 'phat' memory AliasProj perhaps?  Or on the 'phat' producer?

Means: can ask a using node for the set of aliass it uses (without regard to
which input edge), and can insert graph widening Nodes.  Maybe do not need the
field-level nodes because field aliasing info is "perfect".

Phi with "Request memory split" - Shatter "phat" phi into alias Phis, but not
for "has unknown callers".  Can further shatter alias#phis into field#phis.

NewNode-as-closure; during Parsing can add_fld.  But cannot del_fld, even as it
goes out of scope - because of closure can have live uses.  Out-of-scope means
the variable lookup quits succeeding.

Can we have a reachability-analysis for each TypeMem, based on the reaching
TMP+fields, assuming all are read allows max reachability in alias class which
allows max reading ptrs, recursively?
If a alias#+fld doesn't appear in the max-reach, then its not needed in the TypeMem?
Can be different if accessed from different TMPs?  If there is only one, can canonicalize!
Can be ~Scalared in the type, does not show in the used-aliases-on-ask.
Can be recorded as part of the canonicalization?


-------------------------------------
Example needed for updating closure fields directly, bleah.


[Ctrl,AllMem,Val]
 |
 |   NewNode [TStruct,TMP]
  \  [OProj#18,DProj]
   \  |          |   3.1415
   MemMerge[All] /  /
      |         /  /
      Store[#18]
      |  <type is All,18>


Store, direct to MemMerge, cannot bypass on #18 alone, can only bypass if
address pts to prior generator of address.  Works in this case:

[Ctrl,AllMem,Val]
 |
 |   NewNode [TStruct,TMP]
 |   [OProj#18,DProj]    3.1415
 |    |         /       /
 |    Store[#18.z]     /
 |    |  <type is 18>
 |    |
 MemMerge[All]
     |       


Store, direct to OProj,DProj,NewNode, and NO other same-field uses of OProj can
fold; but want independent field folding, so request field split/merge.
Multiple stores will stack back-to-back and serialize.  Probably do not need
THIS level of precision, since field-name-alias is perfect.

[Ctrl,AllMem,Val]
 |
 |   NewNode [TStruct,TMP]
 |   [OProj#18,DProj]
 |   [x][y][z]   |    3.1415
 |    |  |  |    |   /
 |    |  | Store[#18.z]
 |    |  |  |
 |   [FldMerge]
 MemMerge[All]
     |       


Store of a FldProj - must be matching field and alias (or error).
Can "peek" thru for opts.

[Ctrl,AllMem,Val]  3.1415
 |                /
 |   NewNode [TStruct,TMP]
 |   [OProj#18,DProj]
 |   [x][y][z] 
 |    |  |  |  
 |    |  |  |
 |   [FldMerge]
 MemMerge[All]
     |       

"Junk" FldSplit/FldMerge rejoins:

[Ctrl,AllMem,Val]  3.1415
 |                /
 |   NewNode [TStruct,TMP]
 |   [OProj#18,DProj]
 |       |    
 MemMerge[All]
     |       


-------------------------------------




=================================================================================

Hidden variable 'cnt' inside outer closure.
Return two functions in a tuple, one increments cnt, the other gets it.
    > (inc, get) = { cnt=0; ({cnt++;0},{cnt}) }()
    > inc()
    0
    > get()
    1
    > inc()
    0
    > get()
    2
The outer anon fcn returns and exits, but the storage for 'cnt' remains.
'inc' and 'get' can read & write 'cnt', but 'cnt' is otherwise private.

Every ScopeNode turns into a NewNode with variable mappings via TypeStruct,
which grows as new var names appear.  Every fcn call passes in a display with
all parent scopes (the Env).  All var refs become lds/sts against the NewNode/
ScopeNode.  Standard ld/st ops apply, and a NewNode goes dead the normal way-
no other uses.  Last "normal" use goes away when fcn exits, but display based
uses from nested fcns (i.e., a REAL closure usage) might keep alive.

Can I do this without going the ld/st route?  What's so special about threading
memory thru-out?  Or even threading just the NewNode, no aliasing issues... i
think.  In the above inc/get I can call it 3 times, get 3 unrelated counters.
Pass the fcns along, and get them inlined.  So inc1 bumps cnt1, inc2 bumps
cnt2, and inlined side-by-side.  So cnt1,cnt2 memory ops come from the same
anon fcn.  Can call in a loop, have millions of ctrs from the same anon fcn -
which must therefore be the same alias, therefore ld/st required.


Plan B:

Keep ScopeNode, but remove most everything from it.  NewNode makes a Struct
which includes finalness and field names.  But need to allow more fields like
Scope does.  At Scope exit, NewNode allowed to be dead.

NewNode produces a Tuple of TMP+field for every field.  Each ProjNode can go
dead independently, matching dead field goes to XSCALAR.  When all proj fields
die, NewNode goes to XMEM (even with MemMerge use).

Phat memory usage "forgets" fields.  To remove single unused fields, need to
explode out of phat memory.

More precise memory handling: 2 layer split/join:

AliasProj - can follow any whole memory.  Slices out a set of disjoint aliases.
FieldProj - can follow any single alias.  Slices out a set of disjoint fields.
FldMerge - collects complete field updates to form a complete alias type.
MemMerge - collects complere alias updates to form total memory.

NewNode - produces alias# that is further exactly not any other instance of the
same alias#; can be followed by FldProj.
MemMerge - can accept a NewNode input that overlaps with same alias#; NewNode
is now "confused".


Looking for a model where individual fields can go dead.
Looking for a model where pre-wired calls can wire without memory (pure)
or read-only memory (const).

Graph rewrite opts: skinny memory reads from a phat memory: explodes it iff
progress.  skinny write forces parser explosion & rejoin.  ScopeNode mem slot
pts to a phat memory or a MemMerge, which pts to many FldMerges.  Leaves it
exploded as parse rolls forward until sees a usage of phat memory.  Then leaves
the Mem/Fld Merge in the graph, and starts anew after def of phat memory.

Escaped ptrs: if at a phat memory usage we can see no instances of TMP alias#
in the memory or values, we can declare "not escaped", and now remove an alias#
from phat node usage.  To see field escapes, need a backwards prop of field
usages.  Currently thinking has no way to detect lack-of-usage except via (lack
of) graph node edges.  Have to "explode" in the graph all phat into alias#s
into fields, and push the "inflated" graph all about, then do DCE.  Note:
cannot remove dead field if ptr escapes at all, because later parser might use
field.  Strictly ok after removing unknown callers.

FunNode with mem Parm: can skip mem, if mem is not used (pure fcn, common on
many operators).  Can cast TypeObj._news to a limit set, and then only takes
that memory alias set and bypass the rest.  If purely reading memory, still
take in that alias, but RetNode pts to the ParmNode directly.  The cast-to-str
PrimNodes which alloc a new Str do not take memory, but the RetNode produces a
brand new alias which needs to fold into a post-call MemMerge.

Pure: RetNode has a null memory (no pre-call split, no post-call merge).  Parm is missing.
Read subset:  RetNode can be equal to Parm, with subset in Parm type.
Write subset: RetNode & Parm has some (not all alias#s), but not equal to Parm.
All: RetNode has phat, so does Parm - and not equal to Parm.
New: RetNode can include MORE alias#s than the Parm.  Needs a MemMerge.
New factory: Parm is missing.  RetNode takes in some aliases.

Plan B2:  No!

Only keep all memory pre-exploded at the alias/field level.  Leads to huge
count of graph nodes, esp for unrelated chunks of code that just "pass thru".




---
For closures, all local vars actually talk to the scope-local NewNode, which
can grow fields for a time.  Stops growing fields at scope exit.  Local var
uses do Load & Store, which collapse against any NewNode including the scope-
local one.  Scope-local NewNode available for inner scopes - this is the
closure usage, and therefore serialize later stores against inner fun calls.
Requires inner calls always take outer scope NewNode, and later optimize.

MemMerge only used before calls to flatten everything.  Wired calls can switch
to using some aliases instead of all, with the alts aliases going around the
call.

Calls not wired take all of memory, including scope-local News.  Can optimize
against non-escaping aliases.  Wired calls can be more exact.

Funs & Mem Parm - split into separate aliases by usage (not defs).  Pass-thru
memories can be optimized by wired calls: direct from Ret/MemMerge/PhatMemParm.
Bypass in the CallEpi Ideal.  Flag the bypassed aliases in the MemMerge... but
somewhere else, perhaps the FunNode, for better assertions.

Root Scope becomes Root New.  Fields are primitive names.  All "final", except
can replace a prim funptr value with a Unresolved of the same name.




=================================================================================
Old notes from 7/6/2019

Bits-split fail attempts

- Plan A: split 6 into 9,10; remove all 6's via a read-barrier-before-use.
  Fails because do not want read-barrier before equality checks.  i.e., I like
  defining   "isa = x -> meet(x)==x" as
  opposed to "isa = x -> meet(x)==rd_bar(x)"

- Plan B: Visit all types in Nodes&GVN and replace 6s with 9,10s.  Too hard to
  track them all.

- Plan C: Split 6 into 12,13.  Canonicalize even/odd pairs back to parent.
  Numbers grow fast (by powers of 2), but managable in a long.  Comment: "Plan
  C fails, unwinding.  Cannot do even/odd bits-split pairs, because i need to
  be able to walk the expanded bits, but i do not track how much expansion was
  done. Really needs an explicit tree structure. Unwinding the even-odd
  bit-pairs notion."

- Plan D, explicit tree structure.  Didn't write up the failure, only that it
  got complicated.

- Plan A2: 6 "becomes" 6,7 everywhere instantly.  For the local users, this is
  great.  Reset logic between tests is insane (must reset all of Bits,
  BitsAlias, BitsRPC, BitsFun and all TypeMems).  The setup (clinit) is also
  insane, because splitting happens during the clinit so must be ordered
  extremely carefully.  Must be careful to track whether something is a single
  alias#num, or a BitsAlias collection.  Collections split over time and grow,
  but the single number does not.


Now pondering a D2 to avoid the horrible reset in A2.

Explicit trees again.

Tree nodes have an alias# and a type; they are invariant, hashconsed & shared.
They only point "up" to the root.  A "split" call requires a parent, and makes
a hash-consed child.  All children of a Tree node are given unique dense alias
numbers which are unique across the tree.

After the reset, the same node will hand out the same numbers every time, so
Bits collections do not need to be reset.  This can be implemented with a "side
doubling array of ints".  This side array is not part of the Tree Node...
!!!Hey, cheap structure that splits-the-same after reset, so does not need to reset Bits!!!


Still thinking TypeMem might be like a tuple (so no any/all choice)???  Drop it
for now...

TypeMem has all the tree leafs (and so does not need the interior???).
No... all "open" tree nodes have unknown future splits, and need a type for
them.  So TypeMem has the interior nodes; unless i declare "closed" at a level,
and then canonicalization demands I collapse this.  But "closed" not useful for
a long time; only Parse constant syntax strings are closed right now, and
NewNodes making singletons.  Funs and RPCs only closed/singleton if I disallow
cloning for inlining.

Brings me around to: do i need explicit trees or not.  Maybe not: a tree of
numbers only.  If i ask for a new child of a "tree middle node", i get a new
alias#, extend the lazy-ly growing tree.  If parent is from a TypeMem, then
i have its type for the child initial type.  Given a child#, and a TypeMem,
I can lookup the type in the TypeMem by walking the tree structure....

---------------
(1) Drop the "becomes", horrible reset logic.

(2) Keep explicit numbers-only tree.  Array-of-ints for parents.  Array-of-
    Array-of-ints for children.  This is a bare-bones tree structure with dense
    #s for every node that does not change between test resets.

    Array-of-ints for child lengths.  This is reset between tests; the initial
    part matches the 1st init, but the later part is just zeros.  Any child
    reporting a zero is actually lazily filled in by CNT++.

(3) TypeMem maps #s to Types (via NBHML? vs array?).  Missing values just do a
    tree-based lookup.  Still have a above/below notion based on #1.

(4) No "closed" types (yet).  So no need to canonicalize Bits beyond the
    1-vs-many bit patterns.
